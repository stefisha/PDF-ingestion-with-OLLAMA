{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a7662e",
   "metadata": {},
   "source": [
    "# Ollama with PDF Ingestion Project\n",
    "\n",
    "Using Langchain and Chroma, this project demonstrates a local Retrieval-Augmented Generation (RAG) system for ingesting PDF files. The system leverages several tools:\n",
    "\n",
    "- **Langchain**: For managing the entire workflow, including document loading, embedding, and querying.\n",
    "- **UnstructuredPDFLoader**: Used to extract text content from PDF files.\n",
    "- **RecursiveCharacterTextSplitter**: For splitting large text content into manageable chunks.\n",
    "- **Ollama Embeddings**: Converts text into vector embeddings for efficient storage and retrieval.\n",
    "- **Chroma**: A vector database used to store and query the embeddings.\n",
    "- **MultiQueryRetriever**: Enhances retrieval accuracy by generating multiple variations of the user's query.\n",
    "- **ChatOllama**: A local language model (e.g., Mistral) used to generate responses based on the retrieved document context.\n",
    "\n",
    "The pipeline runs entirely offline, ensuring that sensitive documents remain private and secure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5cab6-515e-4dd6-95ae-6393f0c4435c",
   "metadata": {},
   "source": [
    "## PDF Ingestion in the Ollama RAG System\n",
    "\n",
    "### Key Components:\n",
    "1. **UnstructuredPDFLoader**: This component from LangChain is responsible for reading and extracting text content from PDF files. It is designed to handle unstructured data, ensuring that documents of various formats are properly processed.\n",
    "2. **Text Processing**: Once the content is extracted, the RecursiveCharacterTextSplitter tool is used to split the text into smaller chunks. Chunking the text ensures efficient processing and better results during retrieval and embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e8b999-83ba-484a-9b94-f56c201d2036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q unstructured langchain\n",
    "%pip install --q \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0e2f74-7c4b-4665-8d87-bc00656f31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c047f5f",
   "metadata": {},
   "source": [
    "**Loading the PDF**:\n",
    "   The first step in the process is loading the PDF file using `UnstructuredPDFLoader`. The file is specified by its local path, and the loader extracts its content. For example, the following code loads the \"WEF_The_Global_Cooperation_Barometer_2024.pdf\", a document that contains recent research on global cooperation conducted by McKinsey & Company. For practical use, users have the ability to upload their own PDF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "104c0b18-1c06-41a1-a2ca-f9ee23f4f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \"WEF_The_Global_Cooperation_Barometer_2024.pdf\"\n",
    "\n",
    "# Local PDF file uploads\n",
    "if local_path:\n",
    "  loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "  data = loader.load()\n",
    "else:\n",
    "  print(\"Upload a PDF file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cfeca7",
   "metadata": {},
   "source": [
    "We preview the data to see if it is loaded properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38119195-9c91-4e58-aa46-8a74244032af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In collaboration with McKinsey & Company\\n\\nThe Global Cooperation Barometer 2024\\n\\nI N S I G H T R E P'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview first page\n",
    "data[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2faacc1-be29-4d52-a46e-94f5b5b8e728",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dcf2cfe-a7aa-4ecf-85e3-f77b9e850514",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 970aa74c0a90... 100% ▕████████████████▏ 274 MB                         \n",
      "pulling c71d239df917... 100% ▕████████████████▏  11 KB                         \n",
      "pulling ce4a164fc046... 100% ▕████████████████▏   17 B                         \n",
      "pulling 31df23ea7daa... 100% ▕████████████████▏  420 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull nomic-embed-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39aebbf8-92bf-42e5-951e-40bb458852d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED               \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    Less than a second ago    \n",
      "llama3.1:latest            42182419e950    4.7 GB    29 hours ago              \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5394d61f-906b-4776-b8b5-9f0045c76193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q chromadb\n",
    "%pip install --q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83a39856-0cc0-4ebe-8024-9db32455a545",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bad040e2-3abe-4e23-abb9-951b223b9262",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Split and chunk \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb11c92-e732-4a88-8f57-57a19b38e383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 40/40 [01:50<00:00,  2.75s/it]"
     ]
    }
   ],
   "source": [
    "# Add to vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=OllamaEmbeddings(model=\"nomic-embed-text\",show_progress=True),\n",
    "    collection_name=\"local-rag\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eadf50-2f3d-4420-8858-94e9c1682ffa",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec338c4-f282-462f-b0a0-c1899538eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "d1d6ceeb-6883-4688-b923-e771c2b2cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "llm = ChatOllama(model=local_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c436d5cd-5dd0-448c-b5c0-6eddab879c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "71e423dc-f632-46f8-9bec-d74cb268ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    vector_db.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "cb1f308f-8472-4506-9517-d79b61d408f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "915fb18b-cb57-42cf-a9b3-c6f95d3c4e32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what is this about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:01<00:00,  1.15s/it]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 36.58it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 14.64it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 23.34it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 23.14it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' This document is the Insight Report of The Global Cooperation Barometer 2024 by the World Economic Forum in collaboration with McKinsey & Company. It provides an analysis of the state of global cooperation across five pillars: trade and capital, innovation and technology, climate and natural capital, health and wellness, and peace and security. The report examines trends in cooperative actions and their outcomes to determine the overall level of global cooperation in each area. It also includes recommendations for leaders on how to reimagine global cooperation in a new era.'"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(input(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "06c25c1d-d205-409e-90a2-179d0bd7c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:01<00:00,  1.33s/it]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 26.36it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 36.23it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 49.43it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 63.03it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 58.14it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 59.76it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 56.69it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 48.34it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████| 1/1 [00:00<00:00, 51.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The 5 pillars of global cooperation are:\\n\\n1. Trade and capital\\n2. Innovation and technology\\n3. Climate and natural capital\\n4. Health and wellness\\n5. Peace and security.'"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What are the 5 pillars of global cooperation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe79f21-48aa-4820-aa9f-79f3d1a0a519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all collections in the db\n",
    "vector_db.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e72e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
